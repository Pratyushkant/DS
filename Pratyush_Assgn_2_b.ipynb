{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rk-9UZY_xe2"
   },
   "source": [
    "# UMC 301: Applied Data Science and Artificial Intelligence\n",
    "## Assignment 2\n",
    "\n",
    "### Submission instructions:\n",
    "\n",
    "1.   The assignment is to be submitted in ONE single notebook.\n",
    "2.   Submit the .ipynb file and pdf of the same with all cells open through this Teams Assignment.\n",
    "3. If your IISc email ID is < username > @iisc.ac.in, then name the file < username >_Assgn_2. E.g. jonathan_Assgn_1a for email ID jonathan@iisc.ac.in.\n",
    "4. Before submission, execute the ’Restart session and run all’ option from the Runtime/Kernel tab. Verify that there are no errors and that you are getting the output you expect.\n",
    "5. Use the dataset(Q1,Q2) : https://www.dropbox.com/scl/fi/7m0pt1dkmwbkr3byv5r2j/face_images.zip?rlkey=r2gqsdvuyvpqqk4a7bjd9kcvv&st=nj7wl35d&dl=1\n",
    "6. Use the dataset(Q3) : https://www.kaggle.com/datasets/sbaghbidi/human-faces-object-detection/data (Human face object detection)\n",
    "**Data Files for Q1,Q2**\n",
    "\n",
    "**face_images** : All the face images, cropped and aligned. <br>\n",
    "**face_image_attr.csv**: Attribute labels for each image. There are 40 attributes. \"1\" represents positive while \"-1\" represents negative.\n",
    "\n",
    "**Data Files for Q3**\n",
    "\n",
    "**images** : A diverse compilation of human facial images. <br>\n",
    "**faces.csv**: Height, Width are Image dimensions, x0,y0 are upper left corner and x1,y1 are bottom right corner of the bounding box\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaGSaCCg_xe3"
   },
   "source": [
    "# Question 3\n",
    "Train a YOLOv3 model on the dataset (human face object detection) mentioned above for the object detection and evaluate its performance. Provide model output from data point used in Question 2. <br>\n",
    "[Note: Marks will be given based on different experiments and discussion]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7iNjKOCFe2d"
   },
   "source": [
    "The dataset has been downloaded and extracted in the same directory under the name \"archive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "B8ZwG0qSFe2e"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "E_AW-oapGF3T",
    "outputId": "6f47b156-737c-4618-e778-c7e29a464429"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading archive.tar.gz: 100%|██████████| 498M/498M [00:10<00:00, 51.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive.tar.gz downloaded successfully.\n",
      "Extracted archive.tar.gz to the current directory.\n"
     ]
    }
   ],
   "source": [
    "def download_file(url, save_as='archive'):\n",
    "    \"\"\"Download a file from a URL with a progress bar.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Check if the response is successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Unable to download file (Status Code: {response.status_code})\")\n",
    "        return\n",
    "\n",
    "    total_size = int(response.headers.get('content-length', 0))  # Total size in bytes\n",
    "\n",
    "    with open(save_as, 'wb') as f, tqdm(\n",
    "        desc=f\"Downloading {save_as}\",\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            f.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "    print(f\"{save_as} downloaded successfully.\")\n",
    "\n",
    "    # Check if the downloaded file is not HTML\n",
    "    if b'<!doctype html>' in open(save_as, 'rb').read():\n",
    "        print(f\"Error: The downloaded file seems to be an HTML document instead of a valid archive.\")\n",
    "        return\n",
    "\n",
    "def extract_file(archive_file):\n",
    "    \"\"\"Extract ZIP or TAR.GZ files directly in the current directory.\"\"\"\n",
    "    if archive_file.endswith('.zip'):\n",
    "        with zipfile.ZipFile(archive_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall()  # Extract in the same directory\n",
    "        print(f\"Extracted {archive_file} to the current directory.\")\n",
    "\n",
    "    elif archive_file.endswith(('.tar.gz', '.tgz')):\n",
    "        with tarfile.open(archive_file, 'r:gz') as tar_ref:\n",
    "            tar_ref.extractall()  # Extract in the same directory\n",
    "        print(f\"Extracted {archive_file} to the current directory.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Unsupported archive format: {archive_file}\")\n",
    "\n",
    "# Usage\n",
    "url = 'https://www.dropbox.com/scl/fi/fyd8wnz08knqws4xcbpob/archive.tar.gz?rlkey=do19wcjwg94m2jfv9hd6mefvb&dl=1'  # Replace with the correct URL\n",
    "download_file(url, 'archive.tar.gz')\n",
    "extract_file('archive.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-4bLjMdlFe2f",
    "outputId": "0047bb19-d190-4f13-c1b8-2e3aa83ae0ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset saved as train.csv with 2680 rows.\n",
      "Test dataset saved as test.csv with 670 rows.\n"
     ]
    }
   ],
   "source": [
    "def split_csv(file_path, test_size=0.2, random_state=42, train_file='train.csv', test_file='test.csv'):\n",
    "    # Load the dataset from the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Split the data into train and test datasets\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Save the train and test datasets to separate CSV files\n",
    "    train_data.to_csv(train_file, index=False)\n",
    "    test_data.to_csv(test_file, index=False)\n",
    "\n",
    "    print(f\"Train dataset saved as {train_file} with {len(train_data)} rows.\")\n",
    "    print(f\"Test dataset saved as {test_file} with {len(test_data)} rows.\")\n",
    "\n",
    "# Usage example\n",
    "split_csv('archive/faces.csv', test_size=0.2)  # Adjust the test_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hd8I3LFeFe2f",
    "outputId": "5959eed2-e0a7-4864-d16b-6fedf5fa5a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files saved in archive/labels\n"
     ]
    }
   ],
   "source": [
    "def save_labels_from_csv(csv_file, output_dir='archive/labels'):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Iterate over each row in the dataframe\n",
    "    for _, row in data.iterrows():\n",
    "        # remove jpg, leading 0's\n",
    "        image_name = str(row['image_name']).replace('.jpg', '').lstrip('0')\n",
    "\n",
    "        # Prepare the values to write to the text file\n",
    "        values = f\"{row['width']}\\n{row['height']}\\n{row['x0']}\\n{row['y0']}\\n{row['x1']}\\n{row['y1']}\"\n",
    "\n",
    "        # Create the full path for the output text file\n",
    "        output_file = os.path.join(output_dir, f\"{image_name}.txt\")\n",
    "\n",
    "        # Write the column values into the text file\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(values)\n",
    "\n",
    "    print(f\"Text files saved in {output_dir}\")\n",
    "\n",
    "# Usage example\n",
    "save_labels_from_csv('archive/faces.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FOSrqbHOPXlx"
   },
   "outputs": [],
   "source": [
    "def read_and_modify_csv(file_path, output_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the 'image_name' column is treated as a string and wrap the values with quotes\n",
    "    df['image_name'] = df['image_name'].apply(lambda x: f'\"{x}\"')\n",
    "\n",
    "    # Save the modified DataFrame to a new CSV file for verification\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Return the modified DataFrame for further processing\n",
    "    return df\n",
    "\n",
    "# Read, modify, and save both files\n",
    "train_df = read_and_modify_csv('train.csv', 'modified_train.csv')\n",
    "test_df = read_and_modify_csv('test.csv', 'modified_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "CJXvcRRhFe2g",
    "outputId": "d59aa88b-2da9-4de6-aeb1-224cb9b4e48e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded and saved as my_image.jpeg\n"
     ]
    }
   ],
   "source": [
    "def download_image(url, save_as='my_image.jpeg'):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Save the content as a file\n",
    "        with open(save_as, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Image downloaded and saved as {save_as}\")\n",
    "    else:\n",
    "        print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "\n",
    "# Dropbox URL with 'dl=1' for direct download\n",
    "image_url = 'https://www.dropbox.com/scl/fi/fshjsm952vuqm4i0f7mwl/Image.jpeg?rlkey=a5cyxsignlxp05ysfuo064mqs&dl=1'\n",
    "\n",
    "# Usage\n",
    "download_image(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "zxhh3_s5Fe2g",
    "outputId": "d62a5924-8680-4073-ef7d-9df9c885ff63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'Faces'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "# seed_everything() # If you want deterministic behavior\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 12\n",
    "IMAGE_SIZE = 416\n",
    "NUM_CLASSES = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_EPOCHS = 25\n",
    "CONF_THRESHOLD = 0.05\n",
    "MAP_IOU_THRESH = 0.5\n",
    "NMS_IOU_THRESH = 0.45\n",
    "S = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8]\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_FILE = \"/archive/checkpoint\"\n",
    "IMG_DIR = \"/archive/images\"\n",
    "LABEL_DIR = \"/archive/labels\"\n",
    "ANCHORS = [\n",
    "[(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "[(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "[(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "] # Note these have been rescaled to be between [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zbo7LwRAFe2h"
   },
   "outputs": [],
   "source": [
    "scale = 1.1\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=int(IMAGE_SIZE * scale),\n",
    "            min_width=int(IMAGE_SIZE * scale),\n",
    "            border_mode=cv2.BORDER_CONSTANT,value = [0,0,0]\n",
    "        ),\n",
    "        A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
    "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "                A.Affine(shear=15, p=0.5),\n",
    "            ],\n",
    "            p=1.0,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Blur(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.Posterize(p=0.1),\n",
    "        A.ToGray(p=0.1),\n",
    "        A.ChannelShuffle(p=0.05),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
    ")\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT, value = [0,0,0]\n",
    "        ),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
    ")\n",
    "\n",
    "FACE_CLASSES = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3duZwzxYFe2i"
   },
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        img_dir,\n",
    "        label_dir,\n",
    "        anchors,\n",
    "        image_size=416, # By default, we are using 416x416 images\n",
    "        S=[13, 26, 52],\n",
    "        C=1,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file) # Will either be the train.csv or test.csv\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = 416\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.C = C\n",
    "        # If in a single cell, we have an object, then we will have one anchor box responsible for detecting that object\n",
    "        # If multiple anchor boxes detect it, then the one with the highest IOU with the ground truth box will be responsible\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, str(self.annotations.iloc[index, 1]))\n",
    "        print(self.annotations.iloc[index, 1])  # Print the label filename being used\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # Converting from (x, y, w, h, class) to (class, x, y, w, h) format\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=image, bboxes=bboxes)\n",
    "            image = augmentations[\"image\"]\n",
    "            bboxes = augmentations[\"bboxes\"]\n",
    "\n",
    "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
    "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
    "        # 6 = [p0, x, y, w, h, class] where p0 is objectness score (1 if object exists in cell, else 0)\n",
    "        for box in bboxes:\n",
    "            iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors)\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "            x, y, width, height, class_label = box\n",
    "            has_anchor = [False] * 3  # each scale should have one anchor\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "                S = self.S[scale_idx]\n",
    "                i, j = int(S * y), int(S * x)  # which cell\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "                    x_cell, y_cell = S * x - j, S * y - i  # both between [0,1]\n",
    "                    width_cell, height_cell = (\n",
    "                        width * S,\n",
    "                        height * S,\n",
    "                    )  # can be greater than 1 since it's relative to cell\n",
    "                    box_coordinates = torch.tensor(\n",
    "                        [x_cell, y_cell, width_cell, height_cell]\n",
    "                    )\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
    "                    has_anchor[scale_idx] = True\n",
    "\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
    "\n",
    "        return image, tuple(targets)\n",
    "\n",
    "\n",
    "def test():\n",
    "    anchors = ANCHORS\n",
    "\n",
    "    transform = test_transforms\n",
    "\n",
    "    dataset = YOLODataset(\n",
    "        \"/archive/modified_train.csv\",\n",
    "        \"/archive/images/\",\n",
    "        \"/archive/labels/\",\n",
    "        S=[13, 26, 52],\n",
    "        anchors=anchors,\n",
    "        transform=transform,\n",
    "    )\n",
    "    S = [13, 26, 52]\n",
    "    scaled_anchors = torch.tensor(anchors) / (\n",
    "        1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    )\n",
    "    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n",
    "    for x, y in loader:\n",
    "        boxes = []\n",
    "\n",
    "        for i in range(y[0].shape[1]):\n",
    "            anchor = scaled_anchors[i]\n",
    "            print(anchor.shape)\n",
    "            print(y[i].shape)\n",
    "            boxes += cells_to_bboxes(\n",
    "                y[i], is_preds=False, S=y[i].shape[2], anchors=anchor\n",
    "            )[0]\n",
    "        boxes = non_max_suppression(boxes, iou_threshold=1, threshold=0.7, box_format=\"midpoint\")\n",
    "        print(boxes)\n",
    "        plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), boxes)\n",
    "\n",
    "def iou_width_height(boxes1, boxes2):\n",
    "\n",
    "    #Parameters:\n",
    "    #    boxes1 (tensor): width and height of the first bounding boxes\n",
    "    #    boxes2 (tensor): width and height of the second bounding boxes\n",
    "    #Returns:\n",
    "    #    tensor: Intersection over union of the corresponding boxes\n",
    "\n",
    "    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
    "        boxes1[..., 1], boxes2[..., 1]\n",
    "    )\n",
    "    union = (\n",
    "        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
    "    )\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "\n",
    "    #This function calculates intersection over union (iou) given pred boxes\n",
    "    #and target boxes.\n",
    "#\n",
    "    #Parameters:\n",
    "    #    boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "    #    boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "    #    box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "#\n",
    "    #Returns:\n",
    "    #    tensor: Intersection over union for all examples\n",
    "\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    #Does Non Max Suppression given bboxes\n",
    "#\n",
    "    #Parameters:\n",
    "    #    bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "    #    specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "    #    iou_threshold (float): threshold where predicted bboxes is correct\n",
    "    #    threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
    "    #    box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "#\n",
    "    #Returns:\n",
    "    #    list: bboxes after performing NMS given a specific IoU threshold\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n",
    "\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=1\n",
    "):\n",
    "\n",
    "    #This function calculates mean average precision (mAP)\n",
    "#\n",
    "    #Parameters:\n",
    "    #    pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "    #    specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "    #    true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "    #    iou_threshold (float): threshold where predicted bboxes is correct\n",
    "    #    box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "    #    num_classes (int): number of classes\n",
    "#\n",
    "    #Returns:\n",
    "    #    float: mAP value across all classes given a specific IoU threshold\n",
    "\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "\n",
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    cmap = plt.get_cmap(\"tab20b\")\n",
    "    class_labels = FACE_CLASSES\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    for box in boxes:\n",
    "        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
    "        class_pred = box[0]\n",
    "        box = box[2:]\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=2,\n",
    "            edgecolor=colors[int(class_pred)],\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(\n",
    "            upper_left_x * width,\n",
    "            upper_left_y * height,\n",
    "            s=class_labels[int(class_pred)],\n",
    "            color=\"white\",\n",
    "            verticalalignment=\"top\",\n",
    "            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
    "        )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_evaluation_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    anchors,\n",
    "    threshold,\n",
    "    box_format=\"midpoint\",\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "    for batch_idx, (x, labels) in enumerate(tqdm(loader)):\n",
    "        x = x.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        bboxes = [[] for _ in range(batch_size)]\n",
    "        for i in range(3):\n",
    "            S = predictions[i].shape[2]\n",
    "            anchor = torch.tensor([*anchors[i]]).to(device) * S\n",
    "            boxes_scale_i = cells_to_bboxes(\n",
    "                predictions[i], anchor, S=S, is_preds=True\n",
    "            )\n",
    "            for idx, (box) in enumerate(boxes_scale_i):\n",
    "                bboxes[idx] += box\n",
    "\n",
    "        # we just want one bbox for each label, not one for each scale\n",
    "        true_bboxes = cells_to_bboxes(\n",
    "            labels[2], anchor, S=S, is_preds=False\n",
    "        )\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "\n",
    "def cells_to_bboxes(predictions, anchors, S, is_preds=True):\n",
    "\n",
    "    #Scales the predictions coming from the model to\n",
    "    #be relative to the entire image such that they for example later\n",
    "    #can be plotted or.\n",
    "    #INPUT:\n",
    "    #predictions: tensor of size (N, 3, S, S, num_classes+5)\n",
    "    #anchors: the anchors used for the predictions\n",
    "    #S: the number of cells the image is divided in on the width (and height)\n",
    "    #is_preds: whether the input is predictions or the true bounding boxes\n",
    "    #OUTPUT:\n",
    "    #converted_bboxes: the converted boxes of sizes (N, num_anchors, S, S, 1+5) with class index,\n",
    "    #                  object score, bounding box coordinates\n",
    "\n",
    "    BATCH_SIZE = predictions.shape[0]\n",
    "    num_anchors = len(anchors)\n",
    "    box_predictions = predictions[..., 1:5]\n",
    "    if is_preds:\n",
    "        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
    "        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
    "        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n",
    "        scores = torch.sigmoid(predictions[..., 0:1])\n",
    "        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
    "    else:\n",
    "        scores = predictions[..., 0:1]\n",
    "        best_class = predictions[..., 5:6]\n",
    "\n",
    "    cell_indices = (\n",
    "        torch.arange(S)\n",
    "        .repeat(predictions.shape[0], 3, S, 1)\n",
    "        .unsqueeze(-1)\n",
    "        .to(predictions.device)\n",
    "    )\n",
    "    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n",
    "    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
    "    w_h = 1 / S * box_predictions[..., 2:4]\n",
    "    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n",
    "    return converted_bboxes.tolist()\n",
    "\n",
    "def check_class_accuracy(model, loader, threshold):\n",
    "    model.eval()\n",
    "    tot_class_preds, correct_class = 0, 0\n",
    "    tot_noobj, correct_noobj = 0, 0\n",
    "    tot_obj, correct_obj = 0, 0\n",
    "\n",
    "    for idx, (x, y) in enumerate(tqdm(loader)):\n",
    "        x = x.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "\n",
    "        for i in range(3):\n",
    "            y[i] = y[i].to(DEVICE)\n",
    "            obj = y[i][..., 0] == 1 # in paper this is Iobj_i\n",
    "            noobj = y[i][..., 0] == 0  # in paper this is Iobj_i\n",
    "\n",
    "            correct_class += torch.sum(\n",
    "                torch.argmax(out[i][..., 5:][obj], dim=-1) == y[i][..., 5][obj]\n",
    "            )\n",
    "            tot_class_preds += torch.sum(obj)\n",
    "\n",
    "            obj_preds = torch.sigmoid(out[i][..., 0]) > threshold\n",
    "            correct_obj += torch.sum(obj_preds[obj] == y[i][..., 0][obj])\n",
    "            tot_obj += torch.sum(obj)\n",
    "            correct_noobj += torch.sum(obj_preds[noobj] == y[i][..., 0][noobj])\n",
    "            tot_noobj += torch.sum(noobj)\n",
    "\n",
    "    print(f\"Class accuracy is: {(correct_class/(tot_class_preds+1e-16))*100:2f}%\")\n",
    "    print(f\"No obj accuracy is: {(correct_noobj/(tot_noobj+1e-16))*100:2f}%\")\n",
    "    print(f\"Obj accuracy is: {(correct_obj/(tot_obj+1e-16))*100:2f}%\")\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def get_mean_std(loader):\n",
    "    # var[X] = E[X**2] - E[X]**2\n",
    "    channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for data, _ in tqdm(loader):\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def get_loaders(train_csv_path, test_csv_path):\n",
    "    #from dataset import YOLODataset\n",
    "\n",
    "    IMAGE_SIZE = 416\n",
    "    train_dataset = YOLODataset(\n",
    "        train_csv_path,\n",
    "        transform=train_transforms,\n",
    "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
    "        img_dir=IMG_DIR,\n",
    "        label_dir=LABEL_DIR,\n",
    "        anchors=ANCHORS,\n",
    "    )\n",
    "    test_dataset = YOLODataset(\n",
    "        test_csv_path,\n",
    "        transform=test_transforms,\n",
    "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
    "        img_dir=IMG_DIR,\n",
    "        label_dir=LABEL_DIR,\n",
    "        anchors=ANCHORS,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    train_eval_dataset = YOLODataset(\n",
    "        train_csv_path,\n",
    "        transform=test_transforms,\n",
    "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
    "        img_dir=IMG_DIR,\n",
    "        label_dir=LABEL_DIR,\n",
    "        anchors=ANCHORS,\n",
    "    )\n",
    "    train_eval_loader = DataLoader(\n",
    "        dataset=train_eval_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, train_eval_loader\n",
    "\n",
    "def plot_couple_examples(model, loader, thresh, iou_thresh, anchors):\n",
    "    model.eval()\n",
    "    x, y = next(iter(loader))\n",
    "    x = x.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        bboxes = [[] for _ in range(x.shape[0])]\n",
    "        for i in range(3):\n",
    "            batch_size, A, S, _, _ = out[i].shape\n",
    "            anchor = anchors[i]\n",
    "            boxes_scale_i = cells_to_bboxes(\n",
    "                out[i], anchor, S=S, is_preds=True\n",
    "            )\n",
    "            for idx, (box) in enumerate(boxes_scale_i):\n",
    "                bboxes[idx] += box\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        nms_boxes = non_max_suppression(\n",
    "            bboxes[i], iou_threshold=iou_thresh, threshold=thresh, box_format=\"midpoint\",\n",
    "        )\n",
    "        plot_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes)\n",
    "\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ctL8Zr1bFe2k"
   },
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss() # For box prediction\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.entropy = nn.CrossEntropyLoss() # For class prediction\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Constants signifying how much to pay for each respective part of the loss\n",
    "        self.lambda_class = 1\n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_obj = 1\n",
    "        self.lambda_box = 10\n",
    "\n",
    "    def forward(self, predictions, target, anchors):\n",
    "        # Check where obj and noobj (we ignore if target == -1)\n",
    "        obj = target[..., 0] == 1  # in paper this is Iobj_i\n",
    "        noobj = target[..., 0] == 0  # in paper this is Inoobj_i\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        no_object_loss = self.bce(\n",
    "            (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
    "        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
    "        ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "        object_loss = self.mse(self.sigmoid(predictions[..., 0:1][obj]), ious * target[..., 0:1][obj])\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])  # x,y coordinates\n",
    "        target[..., 3:5] = torch.log(\n",
    "            (1e-16 + target[..., 3:5] / anchors)\n",
    "        )  # width, height coordinates\n",
    "        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.entropy(\n",
    "            (predictions[..., 5:][obj]), (target[..., 5][obj].long()),\n",
    "        )\n",
    "\n",
    "        #print(\"__________________________________\")\n",
    "        #print(self.lambda_box * box_loss)\n",
    "        #print(self.lambda_obj * object_loss)\n",
    "        #print(self.lambda_noobj * no_object_loss)\n",
    "        #print(self.lambda_class * class_loss)\n",
    "        #print(\"\\n\")\n",
    "\n",
    "        return (\n",
    "            self.lambda_box * box_loss\n",
    "            + self.lambda_obj * object_loss\n",
    "            + self.lambda_noobj * no_object_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TUBdjwjrFe2k"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Information about architecture config:\n",
    "# Tuple is structured by (filters, kernel_size, stride)\n",
    "# No need to mention padding as in YOLO architecture, padding is always same\n",
    "# Every conv is a same convolution.\n",
    "# List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
    "# \"S\" is for scale prediction block and computing the yolo loss\n",
    "# \"U\" is for upsampling the feature map and concatenating with a previous layer\n",
    "\n",
    "config = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 1], # B is for Residual block and 1 is for number of repeats\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4],  # To this point is Darknet-53\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\", # This is that branch in the image , where we deal with diff scales\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]\n",
    "\n",
    "# Defining the structure of a single CNN block\n",
    "class CNNBlock(nn.Module):\n",
    "\n",
    "    # Initialising the CNN block\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        # bn_act is a flag to use batchnorm and activation\n",
    "        ## **kwargs is used to pass kernel_size, stride, padding etc\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        # Kept bias=False as we are using batchnorm\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    # Defining how the data will flow through the block\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            # If flag for BN and Relu is true, then use them\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)  # Skip connection where we are adding the input to the output\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2 * in_channels, 3*(num_classes + 5) , bn_act=False, kernel_size=1\n",
    "                # 3 is for the number of anchors, so for each cell we have 3 anchor boxes. Each box needs to predict 5 values (4 for bounding box and 1 for objectness score) and num_classes for class prediction\n",
    "                # [p0, x, y, w, h] , where p0 is objectness score\n",
    "            ),\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)  # output of self.pred is (N, 3 * (num_classes + 5), S, S), where N is batch size, S is grid size\n",
    "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "        )\n",
    "    # So the output is dim = N * 3 * S * S * (num_classes + 5)\n",
    "    # For every image in current batch, we have S*S cells, and for each cell we have 3 anchor boxes, and for each anchor box we have num_classes + 5 values to predict\n",
    "    # The output is reshaped to (N, 3, S, S, num_classes + 5) and then permuted to (N, 3, S, S, num_classes + 5) to encode the information in the last dimension\n",
    "    # S = 13, 26, 52 for 3 scales in YOLOv3\n",
    "\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    # By default, we are using 3 channels in input and 20 classes in pascal dataset\n",
    "    def __init__(self, in_channels=3, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []  # for each scale\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor=2),)\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ga5TP-3kFe2l"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    losses = []\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x = x.to(DEVICE)\n",
    "        y0, y1, y2 = (\n",
    "            y[0].to(DEVICE),\n",
    "            y[1].to(DEVICE),\n",
    "            y[2].to(DEVICE),\n",
    "        )\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(x)\n",
    "            loss = (\n",
    "                loss_fn(out[0], y0, scaled_anchors[0])\n",
    "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
    "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
    "            )\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update progress bar\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        loop.set_postfix(loss=mean_loss)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YoloLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    train_loader, test_loader, train_eval_loader = get_loaders(\n",
    "        train_csv_path=\"modified_train.csv\", test_csv_path=\"modified_test.csv\"\n",
    "    )\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_FILE, model, optimizer, LEARNING_RATE\n",
    "        )\n",
    "\n",
    "    scaled_anchors = (\n",
    "        torch.tensor(ANCHORS)\n",
    "        * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    map_values = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        #plot_couple_examples(model, test_loader, 0.6, 0.5, scaled_anchors)\n",
    "        train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
    "\n",
    "        #if config.SAVE_MODEL:\n",
    "        #    save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
    "\n",
    "        #print(f\"Currently epoch {epoch}\")\n",
    "        #print(\"On Train Eval loader:\")\n",
    "        #print(\"On Train loader:\")\n",
    "        #check_class_accuracy(model, train_loader, threshold=config.CONF_THRESHOLD)\n",
    "\n",
    "        if epoch > 0 and epoch % 3 == 0:\n",
    "            check_class_accuracy(model, test_loader, threshold=CONF_THRESHOLD)\n",
    "            pred_boxes, true_boxes = get_evaluation_bboxes(\n",
    "                test_loader,\n",
    "                model,\n",
    "                iou_threshold=NMS_IOU_THRESH,\n",
    "                anchors=ANCHORS,\n",
    "                threshold=CONF_THRESHOLD,\n",
    "            )\n",
    "            mapval = mean_average_precision(\n",
    "                pred_boxes,\n",
    "                true_boxes,\n",
    "                iou_threshold=MAP_IOU_THRESH,\n",
    "                box_format=\"midpoint\",\n",
    "                num_classes=NUM_CLASSES,\n",
    "            )\n",
    "            map_values.append(mapval.item())\n",
    "            print(f\"MAP for epoch {epoch + 1} : {mapval.item()}\")\n",
    "            model.train()\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "    # To load the model in future :-\n",
    "    # model = model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "    # Plotting MAP values for every 3rd epoch\n",
    "    plt.plot(range(3, NUM_EPOCHS + 1, 3), map_values, marker='o')\n",
    "    plt.title('MAP vs Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Average Precision (MAP)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ###################################################################################################################################################################\n",
    "    input_image = Image.open(\"my_image.jpg\")\n",
    "    input_image = test_transforms(image=input_image)[\"image\"]\n",
    "    input_image = input_image.unsqueeze(0)\n",
    "    input_image = input_image.to(DEVICE)\n",
    "\n",
    "    # Get the prediction on the input image using the trained model\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(input_image)\n",
    "\n",
    "    # Get the boxes from the predictions\n",
    "    bboxes = [[] for _ in range(input_image.shape[0])]\n",
    "\n",
    "    for i in range(3):\n",
    "        anchor = scaled_anchors[i]\n",
    "        boxes_scale_i = cells_to_bboxes(out[i], anchor, S=S[i], is_preds=True)\n",
    "        for idx, (box) in enumerate(boxes_scale_i):\n",
    "            bboxes[idx] += box\n",
    "\n",
    "    # Apply non-max suppression\n",
    "    bboxes = non_max_suppression(bboxes[0], iou_threshold=NMS_IOU_THRESH, threshold=CONF_THRESHOLD, box_format=\"midpoint\")\n",
    "\n",
    "    # Plot the image with the predicted bounding boxes\n",
    "    plot_image(input_image[0].permute(1,2,0).detach().cpu(), bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kcWvN4xFe2l"
   },
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKEpS3fKYszM"
   },
   "source": [
    "### References\n",
    "\n",
    "- https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/object_detection/YOLOv3\n",
    "- https://www.geeksforgeeks.org/yolov3-from-scratch-using-pytorch/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
